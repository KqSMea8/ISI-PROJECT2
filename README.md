# ISI-PROJECT2
This project maintained by students is a crawler / search engine for robots.txt files.

Basic functionality of the project:
	1. Download robots.tct from (almost) all polish websites
	2. Allow searching and sorting threw all possible fields (Disallow, Allow, Address, comment, length of the file etc.)
	3. Create measures allowing automatic extraction of "interesting" robot.txt files (length, full links, diffrence from other robots.txt files)
	4. Allow Sorting according to those measurements.

Tasks:
1. Write crawler for all .pl sites and save them to a file / db.
	a. Wikipedia
	b. commoncrawl.org
	c. direct DNS server approach
2. Download all robots.txt files (preferably with wget and some small bash script)
3. Parse each robots.txt file into a "Family Friendly" version, that'd allow easy searching or whatever.
4. Create A frontend for the engine (One google-like inut field?)
5. Create Backend (search engine) (3)
6. Code Measurements (3)

